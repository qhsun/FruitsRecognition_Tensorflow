{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-macosx_10_15_x86_64.whl (46.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 46.3 MB 76.2 MB/s eta 0:00:01    |████████████████████▉           | 30.2 MB 5.1 MB/s eta 0:00:04     |█████████████████████████████▌  | 42.6 MB 76.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from opencv-python) (1.20.3)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp39-cp39-macosx_10_14_x86_64.whl (217.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.5 MB 80 kB/s s eta 0:00:01  |█                               | 7.3 MB 3.5 MB/s eta 0:01:01█████████▍              | 118.3 MB 64.7 MB/s eta 0:00:02     |███████████████████████▏        | 157.7 MB 7.9 MB/s eta 0:00:08\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp39-cp39-macosx_10_10_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 44.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-macosx_10_9_x86_64.whl (13.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.0 MB 69.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 25.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.2.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 31.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jane/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=5c0213057880969df895bb7f6e5a2324f4bdcf56486fb97b7133da851911f2dc\n",
      "  Stored in directory: /Users/jane/Library/Caches/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a09cb044-25a9-4f0f-a908-f1a82e3e8128",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "# from skimage.measure import compare_ssim\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import mlflow\n",
    "# import mlflow.keras\n",
    "# import mlflow.tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import gc\n",
    "from IPython.display import Markdown, display\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will upload the data to Azure Blob Storage and then mount to Azure Databricks DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2b4d3641-743c-498c-827a-8e939a00516a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    # Print with Markdowns    \n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1429b092-71bf-447a-9653-23875fe05b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mount files from Azure blob to Azure Databricks DBFS\n",
    "# dbutils.fs.mount(\n",
    "#   source = \"wasbs://qhsundata516@qhsundata516.blob.core.windows.net\",\n",
    "#   mount_point = \"/mnt/images\",\n",
    "#   extra_configs = {\"fs.azure.account.key.qhsundata516.blob.core.windows.net\": \"snf6WqVRtCaZGHtSto7nCTVG3B+qGdHm0mf07pv4+GYpkVCmDg2l9jIbQiV4n6G/Wr+n8i1S6JNtIYmgKAUQsw==\"})\n",
    "\n",
    "# https://qhsundata516.blob.core.windows.net/qhsundata516/archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datafile was uplated in zipfile format so that we will need to unzip it before accessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb460e48-bea6-4fe4-8716-a648b0c8ced3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(\"/dbfs/mnt/images/archive.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"/dbfs/mnt/images/fruitsimages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4accaece-531b-4a3a-803c-e1b52fafbb65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls /mnt/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35b49c9b-540f-49bd-8a3d-f159e9c609ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%sh unzip /dbfs/mnt/images/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b1d4187-ac49-401b-a8ed-6173e8a18b67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # Add random seed of training for reproducibility\n",
    "\n",
    "# os.listdir() to list all file in the folder\n",
    "def load_images_from_folder(folder,only_path = False, label = \"\"):\n",
    "# Load the paths to the images in a directory or load the images\n",
    "    if only_path == False:\n",
    "        images = []\n",
    "        for filename in os.listdir(folder):\n",
    "            # plt.imread() Read an image from a file into an array.\n",
    "            img = plt.imread(os.path.join(folder,filename))\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        return images\n",
    "    else:\n",
    "        path = []\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder,filename)\n",
    "            if img_path is not None:\n",
    "                path.append([label,img_path])\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6efe7d6-b3f9-431c-8ba9-d30573e8e484",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Apple\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Pitaya\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Pear\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Pomegranate\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Tomatoes\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Kiwi\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Guava\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Plum\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Carambola\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Mango\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/muskmelon\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Banana\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Persimmon\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Orange\n",
      "file_folder + f is /Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/Peach\n",
      "CPU times: user 142 ms, sys: 47.2 ms, total: 189 ms\n",
      "Wall time: 207 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "images = []\n",
    "file_folder =\"/Users/jane/Documents/workspace/FruitRecognition/data/MediumSet/\"\n",
    "\n",
    "# for files saved as png under folder fruitsimages/fruitName, such as fruitsimages/banana, \n",
    "# we will save all png files in the images list directly.\n",
    "# for files saved as png under another folder inside of fruitsimages/fruitName, \n",
    "# we need to generate the path for the inner folder and path\n",
    "# it to load_images_from_folder() to read the image and append to images list.\n",
    "\n",
    "for f in os.listdir(file_folder):\n",
    "    print(f'file_folder + f is {file_folder+f}')\n",
    "    \n",
    "    if \"png\" in os.listdir(file_folder+f)[0]:\n",
    "#         print(os.listdir(file_folder+f)[0])\n",
    "        images += load_images_from_folder(file_folder+f,True,label = f)\n",
    "    else: \n",
    "        for d in os.listdir(file_folder+f):\n",
    "            images += load_images_from_folder(file_folder+f+\"/\"+d,True,label = f)\n",
    "            \n",
    "# # Create a dataframe with the paths and the label for each fruit\n",
    "df = pd.DataFrame(images, columns = [\"fruit\", \"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b98156e-ef2d-4c30-bbcc-c1022fbbbfb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 'Apple', 'Banana': 'Banana', 'Carambola': 'Carambola', 'Guava': 'Guava', 'Kiwi': 'Kiwi', 'Mango': 'Mango', 'Orange': 'Orange', 'Peach': 'Peach', 'Pear': 'Pear', 'Persimmon': 'Persimmon', 'Pitaya': 'Pitaya', 'Plum': 'Plum', 'Pomegranate': 'Pomegranate', 'Tomatoes': 'Tomatoes', 'muskmelon': 'muskmelon'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banana</td>\n",
       "      <td>/Users/jane/Documents/workspace/FruitRecogniti...</td>\n",
       "      <td>Banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tomatoes</td>\n",
       "      <td>/Users/jane/Documents/workspace/FruitRecogniti...</td>\n",
       "      <td>Tomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guava</td>\n",
       "      <td>/Users/jane/Documents/workspace/FruitRecogniti...</td>\n",
       "      <td>Guava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guava</td>\n",
       "      <td>/Users/jane/Documents/workspace/FruitRecogniti...</td>\n",
       "      <td>Guava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>/Users/jane/Documents/workspace/FruitRecogniti...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fruit                                               path     label\n",
       "0    Banana  /Users/jane/Documents/workspace/FruitRecogniti...    Banana\n",
       "1  Tomatoes  /Users/jane/Documents/workspace/FruitRecogniti...  Tomatoes\n",
       "2     Guava  /Users/jane/Documents/workspace/FruitRecogniti...     Guava\n",
       "3     Guava  /Users/jane/Documents/workspace/FruitRecogniti...     Guava\n",
       "4     Apple  /Users/jane/Documents/workspace/FruitRecogniti...     Apple"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df, random_state = 0)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Assign to each fruit a specific number\n",
    "fruit_names = sorted(df.fruit.unique())\n",
    "mapper_fruit_names = dict(zip(fruit_names, [t for t in fruit_names]))\n",
    "df[\"label\"] = df[\"fruit\"].map(mapper_fruit_names)\n",
    "print(mapper_fruit_names)\n",
    "\n",
    "# Visualize the resulting dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7082dd26-c136-4b7b-92c6-9da9d84e87e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the number of pictures of each category\n",
    "# value_counts() return a Series containing counts of unique values in descending order so that the first element is the most frequently-occurring element\n",
    "fruit_valueCounts = df[\"fruit\"].value_counts()\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = fruit_valueCounts.index, y = fruit_valueCounts, palette = \"rocket\")\n",
    "plt.title(\"Number of pictures of each category\", fontsize = 15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ce2fc43-cd8d-4776-bbcb-7f512f85678a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display some pictures of the dataset\n",
    "# fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(15, 15),\n",
    "#                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(plt.imread(df.path[i]))\n",
    "#     ax.set_title(df.fruit[i], fontsize = 12)\n",
    "# plt.tight_layout(pad=0.0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9dbc53b9-cf2d-4cd7-a56a-82e115323eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Resizing images is a critical preprocessing step in computer vision. \n",
    "# Principally, our machine learning models train faster on smaller images. \n",
    "# An input image that is twice as large requires our network to learn from four times as many pixels — and that time adds up. \n",
    "# Moreover, many deep learning model architectures require that our images are the same size and our raw collected images may vary in size.\n",
    "\n",
    "def load_img(df):\n",
    "# Load the images using their contained in the dataframe df\n",
    "# Return a list of images and a list with the labels of the images\n",
    "    img_paths = df[\"path\"].values\n",
    "    img_labels = df[\"label\"].values\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i,path in enumerate(img_paths):\n",
    "        img =  plt.imread(path)\n",
    "        img = cv2.resize(img, (150,150))\n",
    "        label = img_labels[i]\n",
    "        X.append(img)\n",
    "        y.append(label)\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c54f918-faf2-47b8-aed6-92559a0237a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Function to slice the dataset and seperate dataset into training data.\n",
    "# Return a part of the dataframe\n",
    "# For example, if a dataframe has 10 rows and we want to return a part of them\n",
    "# if it is cut in two, it will return the first 5 rows or the last 5 rows depending the part wanted\n",
    "\n",
    "def cut_df(df, number_of_parts, part):\n",
    "# Args:\n",
    "#     df (pandas.DataFrame): The dataframe to cut a part of\n",
    "#     number_of_parts (int): In how many parts should the dataframe be cut\n",
    "#     part (int): The part of the dataframe to return\n",
    "\n",
    "    if part < 1:\n",
    "        print(\"Error, the part should be at least 1\")\n",
    "    elif part > number_of_parts:\n",
    "        print(\"Error, the part cannot be higher than the number_of_parts\")\n",
    "        \n",
    "    number_imgs_each_part = int(df.shape[0]/number_of_parts)\n",
    "#     idx1 = (part-1) * number_imgs_each_part \n",
    "    idx2 = part * number_imgs_each_part\n",
    "    return df.iloc[:idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2059bccb-f565-40cb-a71c-fdb30174dfc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We will be using Sequential() for image classification here.\n",
    "# The Sequential model consists of three convolution blocks (tf.keras.layers.Conv2D) \n",
    "# with a max pooling layer (tf.keras.layers.MaxPooling2D) in each of them. \n",
    "# There's a fully-connected layer (tf.keras.layers.Dense) with 256 units on top of it \n",
    "# that is activated by a ReLU activation function ('relu'). \n",
    "\n",
    "def create_model():\n",
    "    shape_img = (150,150,3) #image_height, image_width, color_channels ---image format\n",
    "    \n",
    "    #instance a Sequential model\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    classifier.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    classifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    classifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    classifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    classifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    classifier.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=shape_img, activation='relu', padding = 'same'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Dense(256) is a fully connected layer with 256 hidden neurons.\n",
    "    classifier.add(Dense(256))\n",
    "    classifier.add(Activation('relu'))\n",
    "    # Dropput() to prevent overfitting.\n",
    "    classifier.add(Dropout(0.5))\n",
    "    \n",
    "    # last layer has n=len(mapper_fruit_names)) dense. Has n output classes.\n",
    "    classifier.add(Dense(len(mapper_fruit_names))) \n",
    "    classifier.add(Activation('softmax'))\n",
    "\n",
    "    classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', 'recall'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20309 validated image filenames belonging to 15 classes.\n",
      "Found 2256 validated image filenames belonging to 15 classes.\n",
      "Found 5642 validated image filenames belonging to 15 classes.\n",
      "CPU times: user 311 ms, sys: 593 ms, total: 904 ms\n",
      "Wall time: 996 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Construct train, validation and test data\n",
    "train_set , test_set = train_test_split(df,test_size=0.2,random_state=17)\n",
    "\n",
    "train_gen = ImageDataGenerator(validation_split=0.1)\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "train_data = train_gen.flow_from_dataframe(\n",
    "    dataframe = train_set,\n",
    "    x_col = 'path',\n",
    "    y_col = 'label',\n",
    "    target_size = (227,227),\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "val_data = train_gen.flow_from_dataframe(\n",
    "    dataframe = train_set,\n",
    "    x_col = 'path',\n",
    "    y_col = 'label',\n",
    "    target_size = (227,227),\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False,\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "test_data = test_gen.flow_from_dataframe(\n",
    "    dataframe = test_set,\n",
    "    x_col = 'path',\n",
    "    y_col = 'label',\n",
    "    target_size = (227,227),\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 18:05:58.171702: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 55, 55, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                61455     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,348,303\n",
      "Trainable params: 58,345,551\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n",
      "CPU times: user 728 ms, sys: 358 ms, total: 1.09 s\n",
      "Wall time: 660 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jane/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Building the sequential model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)), \n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(15, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(lr=0.000001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy','Recall']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "635/635 [==============================] - 2111s 3s/step - loss: 3.7370 - accuracy: 0.2501 - recall: 0.1837 - val_loss: 1.7841 - val_accuracy: 0.4464 - val_recall: 0.3573\n",
      "Epoch 2/40\n",
      "635/635 [==============================] - 2917s 5s/step - loss: 2.5541 - accuracy: 0.3764 - recall: 0.2872 - val_loss: 1.3957 - val_accuracy: 0.5638 - val_recall: 0.4526\n",
      "Epoch 3/40\n",
      "635/635 [==============================] - 6316s 10s/step - loss: 2.0258 - accuracy: 0.4588 - recall: 0.3542 - val_loss: 1.1800 - val_accuracy: 0.6192 - val_recall: 0.4947\n",
      "Epoch 4/40\n",
      "635/635 [==============================] - 4325s 7s/step - loss: 1.6665 - accuracy: 0.5232 - recall: 0.4175 - val_loss: 0.9901 - val_accuracy: 0.6919 - val_recall: 0.5426\n",
      "Epoch 5/40\n",
      "635/635 [==============================] - 3099s 5s/step - loss: 1.4302 - accuracy: 0.5787 - recall: 0.4676 - val_loss: 0.8729 - val_accuracy: 0.7305 - val_recall: 0.5944\n",
      "Epoch 6/40\n",
      "635/635 [==============================] - 2298s 4s/step - loss: 1.2528 - accuracy: 0.6230 - recall: 0.5161 - val_loss: 0.7504 - val_accuracy: 0.7744 - val_recall: 0.6485\n",
      "Epoch 7/40\n",
      "635/635 [==============================] - 2682s 4s/step - loss: 1.1019 - accuracy: 0.6638 - recall: 0.5655 - val_loss: 0.6764 - val_accuracy: 0.7943 - val_recall: 0.6795\n",
      "Epoch 8/40\n",
      "635/635 [==============================] - 2407s 4s/step - loss: 0.9822 - accuracy: 0.6984 - recall: 0.6047 - val_loss: 0.5929 - val_accuracy: 0.8178 - val_recall: 0.7185\n",
      "Epoch 9/40\n",
      "635/635 [==============================] - 2345s 4s/step - loss: 0.8779 - accuracy: 0.7216 - recall: 0.6342 - val_loss: 0.5346 - val_accuracy: 0.8351 - val_recall: 0.7447\n",
      "Epoch 10/40\n",
      "635/635 [==============================] - 2885s 5s/step - loss: 0.7922 - accuracy: 0.7486 - recall: 0.6685 - val_loss: 0.4810 - val_accuracy: 0.8471 - val_recall: 0.7708\n",
      "Epoch 11/40\n",
      "635/635 [==============================] - 3161s 5s/step - loss: 0.7164 - accuracy: 0.7702 - recall: 0.7007 - val_loss: 0.4424 - val_accuracy: 0.8604 - val_recall: 0.7877\n",
      "Epoch 12/40\n",
      "362/635 [================>.............] - ETA: 24:18 - loss: 0.6768 - accuracy: 0.7838 - recall: 0.7200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r2/3p477ft942zg5qh1ty8s_b2m0000gn/T/ipykernel_22038/3954044138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,epochs=50,validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d01cb14-a42f-465d-88f8-d1bf61937476",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def from_categorical(lst):\n",
    "    \"\"\"\n",
    "    Inverse of to_categorical\n",
    "    Example: [[0,0,0,1,0], [1,0,0,0,0]] => [3,0]\n",
    "    \"\"\"\n",
    "    \n",
    "    lst = lst.tolist()\n",
    "    lst2 = []\n",
    "    for x in lst:\n",
    "        lst2.append(x.index(max(x)))\n",
    "    return lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e51cfb61-f7fe-433e-859f-39f22696aef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_stats(y_test, pred):\n",
    "# Display prediction statistics\n",
    "    print(f\"### Result of the predictions using {len(y_test)} test data ###\\n\")\n",
    "    y_test_class = from_categorical(y_test)\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_test_class, pred))\n",
    "    print(\"\\nConfusion Matrix:\\n\\n\")\n",
    "    print(confusion_matrix(y_test_class, pred))\n",
    "    print(\"\\n\")\n",
    "    printmd(f\"# Accuracy: {round(accuracy_score(y_test_class, pred),5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcb76adf-d102-4272-9be2-6ef24a08d06b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_training(model):\n",
    "    history = pd.DataFrame(model.history.history)\n",
    "    history[[\"accuracy\",\"val_accuracy\"]].plot()\n",
    "    plt.title(\"Training results\")\n",
    "    plt.xlabel(\"# epoch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28198062-443e-46fa-b2ca-50a1317873ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "hists = []\n",
    "\n",
    "divisor = 5\n",
    "\n",
    "start_time = time.time()\n",
    "# X_train, y_train = load_img(cut_df(df,divisor,1))\n",
    "X, y = load_img(df)\n",
    "X = X/255\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "# train_df,test_df = train_test_split(df[['path','fruit']].sample(frac=0.05,random_state=0), test_size=0.2,random_state=0)\n",
    "# Converts a class vector (integers) to binary class matrix. \n",
    "# A binary matrix representation of the input (y_train), which can be used with categorical_crossentropy\n",
    "y_train = to_categorical(y_train) \n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# If the model doesn't increase its prediction accuracy on the validation data after \n",
    "# 20 epochs, stop the training and take the best MODEL.\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=5, callbacks=callbacks, validation_split = 0.1, verbose = 1)\n",
    "hists.append(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc6cb9af-af6f-44f1-bb64-55f6224a508d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Run the garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b56432cf-a43f-4ccc-b525-95483e38c2ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_model = time.time() - start_time\n",
    "print(f\"Time to train the model: {int(time_model)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03ffdbe6-3a6c-4e18-9c97-e2ac51545c13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "frutiRecognition_python",
   "notebookOrigID": 1642627161051042,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
